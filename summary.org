#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-bigblow.setup
#+TITLE: Bachelor Thesis Dimensionality Reduction
#+AUTHOR: Massimiliano Falzari (s3459101)
#+EMAIL:     m.falzari@student.rug.nl
#+KEYWORDS:  autoencoder,dimensionality reduction,latent space
#+LANGUAGE:  en
* Overview
** Project scope
  The project aim to build an autoencoder for dimensionality
  reduction. In particular, this will be used to hopefully enhance the
  performance of a DRL algorithm for opensim-rl simulation and to
  enhance the ability to generalize in different environment.
  In this project different type of Autoencoders will be tested.

** Index:
*** [[*Lines of Thoughts][Lines of Thoughts]]
A kinda of overview on what the thesis is about (e.i. Autoencoders and
dimensionality reduction)
*** [[*Papers][Papers]]
A list of all the background literature found. For each of paper there
is a short description of the aims and  the results.
*** [[*Autoencoders][Autoencoders]]
Contains a list of implemented and not autoencoders
*** [[*Ideas for Hyperparameters][Ideas for Hyperparameters]]
Contains some ideas for the hyperparameters fitting and some maybe
clever ideas
*** [[*Code][Code]]
Contains the git repo and the link for the code documentation
*** [[*Todos][Todos]]
Contains a list of different type of Todo
*** [[*Presentations][Presentations]]
Contains all the presentations done or in progress
*** [[*Contacts][Contacts]]
Self explanatory
* Lines of Thoughts
** DONE Dimensionality Reduction
   CLOSED: [2022-02-21 Mon 10:41]
   The concept of dimensionality reduction is quite
   straigthforward. The idea is to reduce the number of
   dimensions/features while retaining maximum information.
   Even thought the definition is quite simple being able to perform
   such transformation is not trivial.

   Ideally, the reasons for performing such proceses are:(*Note* this
   list is not complete but gives a general overview)
   + Avoid the curse of dimensionality
   + Reducing potential overfitting of further processing
   + Reducing computation time of further processing
   + Reducing storage space
   + Plotting
   + Noise removal
   + Removing Correlated features
   + Removing redundant features

   There are several methods that are able to perform this
   transformation. They are usually divided in 2 category:
   + Linear methods
   + Non-Linear methods

   Of course there can also be other type of categorizations
   (e.g. feature selection, feature extraction,Neural,Manifold based,Local methods etc.)
   In the following sections we will present roughly 2 approaches  per
   category.

   #+begin_info
   *NOTE* we will only focus on unsupervised methods since they are
   the most suitable for real-life situation where having labeled data
   is hard and expensive.
   #+end_info

** DONE Autoencoder
   CLOSED: [2022-02-21 Mon 10:41]
An Autoencoder is a special network architecture which approximate two
different function *encode* and *decode* such as:
$$decode(encode(\hat{X})) = \hat{X}$$
#+begin_info
*Note*  most of the the time is not an \equal but an \approx
#+end_info

The network is therefore composed by two different sub networks. An
Encoder which can be defined as:
$$encode \rightarrow \mathbb{R}^n \times \mathbb{R}^m $$
And a Decoder which can be defined as:
$$decode \rightarrow \mathbb{R}^m \times \mathbb{R}^n $$

There two constraint to this two function. The first one is that
*decode* must be approximately the inverse of the *encode*[fn:ae_saferl]. The second one is that
$$ m << n $$
#+begin_info
*NOTE* when the second constraint is sudisfacted, the autoencoder is
 cosidered an undercomplete autoencoder. However, every time we will
 use the autoencoder word we will refer to undercomplete autoencoder.
#+end_info
The second constraint is an architectural one, meanwhile the first one
is a functional constraint which will be achived after the network is
trained.

The error function is therefore a reconstruction error or distance
measure between the input and outuput.

The layer between the Encoder and the Decoder express what is usually
knonw as *Latent space* which dimensionality is $\mathbb{R}^m$.

We will from now on refer to the *Latent space* as $\hat{z}$.
For clarity we can rewrite the above formulas as:
$$encode(\hat{X}) = \hat{z}$$
$$decode(\hat{z}) \approx \hat{X}$$
#+CAPTION: The Autoencoder Structure
#+attr_html:  :width 600em
[[file:resources/autoencoder.png]]
As Wang stated [fn:ae_dimred]
#+begin_verse
Auto-encoder can be seen as a way to transform representation.
#+end_verse

** DONE PCA
   CLOSED: [2022-02-21 Mon 10:41]
   Principal Component Analysis(PCA) is a linear technique. It is
   probably one of the most used methods because of its reliability
   and explainability.
   Conceptually, PCA find the directions of maximum variance in the
   data and project it into a new space with fewer dimension than the
   data

   The crucial point of PCA is to find the Principal Component of the
   data which are therefore completely uncorrelated while maintaining
   most of hte variability of the data. *Note* The Principal
   Components are selected based on the explained variance.
*** Assumptions
    + Linear dimensions (i.e. the variables in the dataset must combine in a linear manner)
    + approximately normally distributed data
** DONE LDA
   CLOSED: [2022-02-21 Mon 11:10]
   Linear Discriminant Analysis (LDA) is a linear method.
   In a nutshell, we want to find a new subspace to project the data
   in order to maximize classes separability.

   The idea to measure such separability is to maximize the difference
   between the mean of each class while minimizing the spread within
   the class.

   The main disadvange is that LDA have good performance only if the
   dataset is Normally distributed.
*** Assumptions
    + Normally distributed data
    + Linear combination of features
** DONE LLE
   CLOSED: [2022-02-21 Mon 11:25]
   Locally Linear Embedding (LLE) is a non-linear methods.
   Conceptually it aims to discover the underline non-linear structure
   of the data set while preserving the distance within local
   neighborhoods.

   *TODO* add image lle.jpg

   This techinique is a 3 steps procedure:
   1. Uses a KNN approach to find the k nearest neighbors of every
      data point.
   2. Approximates each data vecotr as a weighted linear combination
      of its k-nearest neighbors. (*Note* all data point which are not
      in a particular neighborhoods have 0 weight)
   3. Computes the wieghts that best reconstruct the vectors from its
      neighbors
*** Assumptions
    + Euclidean distance to compute k-nearest neighbors
    + Quite sensible to outliers and noise
** DONE Isomap
   CLOSED: [2022-02-21 Mon 11:39]
   Isometric Mapping (Isomap) is a non-linear methods which belong to
   the category of Manifold Learning.

   Ideally it is quite similar to LLE, however, the crucial objective
   of this mapping is to maintain a geodesic distance between two
   points.

   #+begin_info
   *Note* Geodesic is the shortest path between two points on the
    surface itself. This is why Isomap is considered a Manifold
    Learning method
   #+end_info

   This techinique is also defined by a 3 steps process:
   1. Construct a neighborhoods graph (equivalent to the first step of
      LLE)
   2. Compute shortest path between points (using either Dijkstra's or
      Floyd-Warshall algorithm)
   3. Construct a d-dimensional embedding by a partial eigenvalue
      decomposition (i.e. taking the d largest eigenvalues of the
      kernel)

*** Assumptions
    + Computational intensive
    + Euclidean distance for k-nearest neighbors





** DONE Intermezzo
   CLOSED: [2022-02-21 Mon 12:56]
   So, we have rapidly been through classical and non dimensionality
   reduction technique. The main focus of this thesis, though, is to
   perform what is usually refered as Representation Learning.

   Of course, it is quite trivial to see how Representation Learning
   and Dimensionality reduction are strictly related.

   Indeed, a representation usually has fewer dimension than the
   original input. A good representation also should maintain the
   most important information/features of the input space.

   Therefore, the two branch are strictly related. However, it is
   important to notice that a good Dimensionality reduction method
   does not always produce a good representation (by good we mean that
   it has all the important features needed to learn a mapping between
   states and actions)

   For this reason, this thesis will mainly focus on Autoencoders
   technique to perform dimensionality reduction since they give a
   good tradeoff between process flexibility  and accuracy.

   It is also crucial to notice that the literature indicate that
   usually Autoencoders-based latent space (or embeddings) outperforms
   other dimensionality reduction technique when the latent space is
   used as input in an RL-based framework
   *TODO* add references.

   Before jumping into more advanced Autoencoders-based techinique we
   will brefly introduce MDPs Generalization.
   This is another important point in the thesis since, the two main
   objective of constructing low dimensional latent space for an RL
   algorithm are:
   + Faster and more stable convergence
   + Better Generalization property

   The first point seems quite intuitive. Having a low dimensional
   state space should results in a faster and more stable convergence
   since the RL algorithm needs to  learn a mapping from a low dimensional
   state space to action which should be easier than learning a mapping from
   a high dimensional state space to action.

   Another intersting point made in (DARLA paper) is that all Deep
   Reinforcement Learning (DRL) algorithm implicitly learns a first
   mapping from high dimensional state space to low dimensional state
   space and then the maps this low dimensional state space to
   action.
   Therefore, by performing dimensionality reduction we take away the
   concern of learning a good representation from the RL algorithm
   which therefore will only focus on learning a mapping from state to
   action directly.

   Other valueable property of doing such a process are described in
   the next chapter.

** DONE MDPs Generalization
   CLOSED: [2022-02-21 Mon 12:11]
   For formal description of this concept look up at (DARLA reference)
   The idea though is quite intuitive. Let assume that we have a
   =natural world= from which we are able to sample MDPs. The crucial
   characteristic of these MDPs is that they all do have the same
   action space but they have differences between the state spaces.
   However, since we are sampling these MDPs from the same =natural
   world= these state spaces must have some structural similiarity
   (i.e. isomorphisms)

   Therefore, in order to have good generalization property, we need
   to construct a good representation which aims to represent the
   state space of the =natural world=. In order to do so, we cannot
   leave this concern to an DRL methods for the following reason.
   Since DRL is maximizing some objective, it make sense that the best
   representation is the most MDP-entangled one and therfore is the
   one that is guided by the learning process to learn. (For reference see
   DARLA) Therefore, if we do not move this concern outside the DRL we
   will have poor generalization cability.

   Here, Dimensionality reduction methods such as Autoencoders comes
   to rescue. Since they do not maximize the same objective as the DRL
   we can guide the process of learning a representation as we
   please.
   Moreover, we will discover in the nexts chapters how it is crucial
   to aims for disentagled representation.

   The main downfall of moving the concern of learning a
   representation outside DRL is that we need to becarefull of the
   what kind of dataset we use to train the autoencoders. It is
   crucial that the dataset has big variability and covers most of the
   "visible" state space. This is because, a lot of autoencoders
   architecture have weird/undefined behaviour in point of the space
   not explored during training which is not desirable.

   Other potential downfalls are:
   + Increase overall computation time (not always true though)
   + Risk of losing important information for the DRL algorithm
   + Non-trivial definition of AE-hyperparameters

   Since usually the AE objective is centered on the reconstruction
   error it is not trivial to focus on learning usefull representations
   as opposed to learning representations which are based on the
   ability of the decoder to achive better reconstruction errors.
   Therefore, tradeoff must be made in order to achive usefull
   representations for RL

** TODO Disentagled Representations
** TODO Adversarial Autoencoder
** TODO Variational Autoencoder
** TODO \beta-VAE
** TODO Info VAE
** INPROGRESS Random thought
   Based on (towards ) why not training the AE and RL toghether.
   an idea can be to mix the two objective toghether by a sum (for a
   trivial case)
   other ideas can be to scale the objective of the AE based on time
   another idea can be to scale the objective of the AE based on how
   "good" the current representation is. For example we can use on of
   the disentanglement metrics described in the papers .
   Another interesting ideas is to use some metric of the
   representation to guide the exploration policy of the RL
   agent. Something like: There are points/ areas of the latent space
   which we did not explore yet or that maybe we have a low "bias" (we
   just encounter those states just few times)

   Seems that active perception is quite crucial to learn a good and
   usefull representation of the world! and more importantly to learn
   the invariant of the world.
   Maybe this has something to do with active learning and GFlow net
   by Benjo! More investigation is needed.



* Ideas for Hyperparameters
  Since we will have to fit quite a lot of hyperparameters we tried to come
  up with some cleaver ideas to remove some  of these
  hyperparameters.

** Number of neurons per layer

  The first hyperparameters we would like to remove is the number of
  neurons per layer. Since we are doing an autoencoder and therefore
  we are trying to find a compression function *f* we can assume that
  the number of neurons per layer is defined by some kind of function
  *h* that given the number of layers *N*, the numeber of dimension of
  the input *I* and the final number of dimension of the latent space
  *Z* returns the number of neurons for a specific layer.
  This function can be either linear or non-linear. The first
  intuition is that if *h* is linear it should be somewhat easier to
  learn a good compression function *f*. However, until now, we do not
  have any mathematical backgroud for this intuition! We need to do
  more research!

  The first possible implementation of this function is defined as
  follows: (*n* is the index of the layer for which we are trying to
  find the number of neurons)

  $$n_1 = I$$
  $$n_N = Z$$
  $$n_i = n_{i+1}*\lambda$$
  From these equations we can find out the equation which define the
  value of \lambda quite intuitevely.
  $$ \lambda = \sqrt[N-1]{\frac{I}{Z}} $$
  Now we can derive the number of layers given the
  number of neurons for the first and last layer
  $$ n_i = n_N * \prod_{x=1}^{N-i}  \lambda $$
  $$ n_i= n_N * \lambda^{N-1}$$
  Substitute \lambda with the prievious found equation.
  $$ n_i = n_N *  \left(\sqrt[N-1]{\frac{I}{Z}}\right)^ {N-i} $$
  $$ n_i = n_N * \left(\frac{I}{Z}\right)^{\frac{N-i}{N-1}} $$
  $$ n_i = Z * \left(\frac{I}{Z}\right)^{\frac{N-i}{N-1}} $$

* ML Pipeline
#+BEGIN_EXPORT html
<iframe src="./resources/ML-pipeline.pdf" width="100%" style="height:50em" align="center"> </iframe>
#+END_EXPORT
* Autoencoders
** DONE Vanilla
CLOSED: [2022-01-21 Fri 15:24]
    The vanilla autoencoder is the classical one. Composed by an
    encoder and a decoder without any kind of constriction.
** DONE VAE
CLOSED: [2022-01-21 Fri 15:24]
    The Variational autoencoder is a modified version of a vanilla AE
    which forces the distribution of the latent space to be a
    gaussian.
** DONE AVB
CLOSED: [2022-01-21 Fri 15:24]
    Adversarial Variational Bayes is a relatively new ideas which
    exploits some Bayesian concept to force a particular latent space
    distribution. All is done in an adversarial environment.
** HOLD B-VAE
   Quite interesting if focus is on transfer learning and disentangled representations

** HOLD InfoVae

* Papers
** DONE Dimensionality reduction
   CLOSED: [2022-02-21 Mon 09:49]

*** Common knowledge resources
+ [[https://www.analyticsvidhya.com/blog/2021/06/dimensionality-reduction-using-autoencoders-in-python/][autoencoder dim red python]]
+ [[https://towardsdatascience.com/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b][dimredtechniques]]
+ [[https://iq.opengenus.org/applications-of-autoencoders/][applications autoencoders]]
+ [[https://towardsdatascience.com/how-to-mitigate-overfitting-with-dimensionality-reduction-555b755b3d66][reduce overfitting dim red]]
+ [[https://github.com/amir-abdi/disentanglement-pytorch][disentaglement pytorch autoencoders]]

*** DONE Autoencoders:
    CLOSED: [2022-02-21 Mon 09:49]
**** DONE \beta-VAE: Learning basic visual concepts with a constrained variational framework[fn:b-vae]
     CLOSED: [2022-02-18 Fri 12:58]
[fn:b-vae]
[[file:background/beta_vae_learning_basic_visual.pdf]]


***** Summary
      + introduces \beta-VAE
      + shows how to achive better disentangled representations
      + introduces a new hyperparameter (\beta) which forces the
        posterior to be closer to the isotropic unit Gaussian (prior)
      + Introduces a new metric for measuring the disentanglement
        (independece + interpretability)
      + reconstruction error should not be used to discriminate
        between AE (or at least it should be the only factor) see
        conclusions
      + Interesting references (usefull for thesis)

**** DONE Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks [fn:avb]
     CLOSED: [2022-02-19 Sat 11:37]
[fn:avb]
[[file:background/mescheder2017avb.pdf]]

***** Summary
      + Introduces AVB
      + Adversarial procedure
      + focuses on giving better flexibility to the normal VAE
        procedure
      + Quite interesting approach however,does not focuses on
        disentagling the representation so, even tho, it achieve on
        average better results than a normal VAE maybe it is not
        suitable in a RL framework. Testing is need to asses the
        performance.


***** Resources
[[https://chrisorm.github.io/AVB-pyt.html]]

**** DISCARDED Generalized Autoencoder: A Neural Network Framework for Dimensionality Reduction[fn:gen_ae]
     CLOSED: [2022-02-18 Fri 15:03]
[fn:gen_ae]
[[file:background/Wang_Generalized_Autoencoder_A_2014_CVPR_paper.pdf]]

***** Summary
      + Interesting approach to dimensionality reduction which tries
        to focus on the main downfall of the AE
      + However, does not look like anymore progress was done in this
        direction
      + Therefore seems to be a bit too much to try for the thesis
      + That's why it was *DISCARDED*

**** DONE Auto-Encoding Variational Bayes [fn:original_vae]
     CLOSED: [2022-02-18 Fri 14:00]
[fn:original_vae]
[[file:background/Auto-Encoding_Variational_Bayer.pdf]]

***** Summary
      + Paper which introduce the theory behind VAE
      + Variational inference
      + Highly teoretical and matematically
      + *Crucial* to understand what,how and why we will want
        something like a VAE
***** Resources
      [[https://www.youtube.com/watch?v=HxQ94L8n0vU&t=31s][Variational Inference intuitions and derivations]]

**** DONE InfoVAE: Balancing Learning and Inference in Variational Autoencoders [fn:infovae]
     CLOSED: [2022-02-19 Sat 14:15]
[fn:infovae]
[[file:background/infovae.pdf]]
***** Summary
      + Shows crucial down fall of the ELBO objective.
      + Introduces a new objective.
      + Quite interesting cause it tries to balance the mutual
        information between the X and Z while trying to force the
        posterior distribution to a family of distribution
        (e.g. Guassian)
      + It is a generalization of the \beta-VAE, VAE and Adversarial
        AE
      + Also shows how it is possible to change the divergence metric
        in this new objective function (the only requirement is that
        it must be a strict divergence metric (i.e. D(p,q) = 0 iff
        p(x) = q(x)))
      + This new objective also focuses on learning disentagled
        representations
      + REALLY INTERESTING in particular in an RL framework since
        it parametrize both mutual information and disentaglement.

**** DONE Adversarial Autoencoders [fn:aae]
     CLOSED: [2022-02-18 Fri 15:28]
[fn:aae]
[[file:background/AdversarialAutoencoder.pdf]]


     #+CAPTION: Adversarial Autoencoder structure
     #+attr_html:  :width 600em
     [[file:resources/AAE.png]]

***** Summary
      + introduces formally the AAE
      + The main difference between VAE and AAE is that the KL term
        (or cross entropy) is replaced with a discriminator network
	to force and adversarial learning process
      + Moreover, in constrast to VAE, we do not need to have a
        functional form of the posterior distribution we want to
        force. We just need to be able to sample from it.
      + The objective of the AE is to both minimize the reconstruction
        error and to fool as best as possible the discriminator.
      + The discriminator is used to discriminate between the wanted
        posterior and the actual latent space distribution

      + Quite interesting however, does not look like it focus on
        disentangled representations but on the reconstruction error
        which maybe is not suitable if the main point is to use it
        within an RL framework. However, it can be interesting to test
        it.


**** DONE Understanding disentagling in \beta-VAE [fn:understand_bvae]
     CLOSED: [2022-02-19 Sat 15:53]
[fn:understand_bvae]
[[file:background/understand_beta-VAE.pdf]]

***** Summary
      + Give some intuitions on why \beta-VAE achive disentaglement
      + Show some more formal derivations on the why
      + Introduces a modification on the "standard" \beta-VAE
      + Quite interesting, however, also here there seems to not be
        any research on the applications with RL

**** DONE Learning representations by maximizing mutual information in variational autoencoders [fn:infomax_vae]
     CLOSED: [2022-02-19 Sat 16:22]
[fn:infomax_vae]
[[file:background/infomax-vae.pdf]]

***** Summary
      + Quite interesting new architecture (similar approach
        to [fn:infovae])
      + Again mark the fact that ELBO and KL aims to decrease the
        mutual information between the input and the latent
        representation which can results in quite bad representations
      + for future research!

**** HOLD The information autoencoding family: A lagrangian Perspective on latent variable generative models [fn:lagrangian_vae]
[fn:lagrangian_vae]
[[file:background/lagrangian-vae.pdf]]

***** Summary
      + Quite advanced mathematically maybe for the future research

**** HOLD CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models 
     [[file:background/CasualVAE.pdf]]

***** Summary
      + The concept of causuality seems pretty interesting in
        particular with respect to phisics and real environments,
        however, left for future research


*** DONE Dimensionality reduction:
    CLOSED: [2022-02-21 Mon 09:49]
*** DONE Towards a Definition of Disentangled Representations
[[file:background/disentangledrepresentation.pdf]]

**** Summary
      + Gives a formal definition of what a Disentangled
        representation is
      + *Really crucial paper*
      + Idea based on symmetry transformations (like in physics)
      + Mainly based on group theory
      + Really good references (Look at the underlined ones)
      + Interesting idea about active perception!

**** DONE Auto-encoder based dimensionality reduction [fn:ae_dimred]
CLOSED: [2022-01-21 Fri 15:27]
[fn:ae_dimred]
[[file:background/Autoencoder_based_dimensionality_reduction.pdf]]


*Contributions*
#+begin_verse
We start from auto-encoder and focus on its ability to reduce
the dimensionality, trying to understand the difference between
auto-encoder and state-of-the-art dimensionality reduction
methods. The results show that auto-encoder indeed learn
something different from other methods.
#+end_verse

#+begin_verse
We preliminarily investigate the influence of the number of
hidden layer nodes on the performance of auto-encoder on
MNIST and Olivetti face datasets. The results reveal its possible
relation with the intrinsic dimensionality.
#+end_verse
***** Summary
Shows comparison of Autoencoder and other dimensionality reduction
methods (e.g. PCA,LLE) Notable results: Autoencoder different than
other dimensionality reduction. Potentially detect repetitive
structures. Dimensionality of the *Latent space* is best when it
maches the intrinsic dimensionality of the dataset.

***** Opinions
This clearly shows how Autoencoder can be essentially different and
more usefull than other dimensionality reduction methods. This
consolidate the choice of using autoencoders in the thesis.

**** DONE Dimensionality Reduction of SDSS Spectra with Variational Autoencoders [fn:dimred_vae]
CLOSED: [2022-01-21 Fri 16:40]
[fn:dimred_vae]
[[file:background/dim_red_vae.pdf]]

***** Summary
Show how AEs were already used in astrony for different dimensionality
reduction/classification task with success. Moreover, it aims to
address the limitation of PCA using VAE. Results show how on this
dataset (SDSS sloan digital sky survey) the autoencoder outperforms
PCA in particular with low dimension latent space(or component for
PCA)
They mainly use InfoVAE[fn:infovae],a variant of the VAE, focused on trying to
disentangle (e.i. force mapping different inputs to disjoint distribution ) the different latent space dimensions.

**** DISCARDED Dimensionality reduction for EEG-based sleep stage detection: comparison of autoencoders, principal component analysis and factor analysis [fn:dimred_comp]
[fn:dimred_comp]
[[file:background/dim_red_comparison.pdf]]


It was *DISCARDED* because it contains too specific content and the
comparison between algorithms has multiple steps and variables which
are highly specific to the task. Therefore I do not think it should be
used.
**** DONE A deep adversarial variational autoencoder model for dimensionality reduction in single-cell RNA sequencing analysis [fn:dimred_vae_rna]
CLOSED: [2022-01-23 Sun 15:49]
[fn:dimred_vae_rna]
[[file:background/VAE_dim_red_RNA_sequencing.pdf]]


#+CAPTION: Adversarial Variational Autoencoder with dual matching
#+attr_html:  :width 600em
[[file:resources/The-novel-architecture-of-an-Adversarial-Variational-AutoEncoder-with-Dual-Matching.png]]

***** Summary
It introduces a novel Adversarial autoencoder architecture named
AVE-DM (Adversarial Variational autoencoder with dual matching )
The main difference between this new architecture and the prievious
proposed Adversarial autoencoders[fn:aae] is that it has 2
discriminator (hence the name dual matching).
Main Results: Shows how AVE-DM outperforms other state-of-the-art
methods such us PCA, UMAP (Uniform Manifold Approximation and
Projection),
t-SNE (T-distributed sochastic neighbor embedding ) etc.
Note: The interesting part of the data is that it have dropout event
(the reason for it is quite technical and specific to RNA
sequencing). This dropout event are zero expression measurament that
can be either biological or technical. This phenomenon result in poor
results for methods such us PCA or t-SNE.
*** DONE AE + RL:
    CLOSED: [2022-02-21 Mon 09:49]
**** DONE VARL: a variational autoencoderâ€‘based reinforcement learning Framework for vehicle routing problems [fn:varl]
CLOSED: [2022-01-23 Sun 16:30]
[fn:varl]
[[file:background/Wang2021_Article_VARLAVariationalAutoencoder-ba.pdf]]


*Quotes*
#+begin_verse
It inherits the idea of variational inference to use a distribution to approximate the posterior distribution[fn:original_vae]. The
diference is that VAE considers the posterior distribution
of all data simultaneously and approximates each posterior
distribution with distribution, minimizing KL divergence.
#+end_verse


#+begin_verse
It has many advantages,
including fast training, stability, and so on, so it has a wide
range of theoretical models and industry applications.
#+end_verse

***** Summary
Introduces a new variational framework  for combinatorial optimization
(e.g. TSP). Introduces Variational inference and VAE. Propose VARL
(Variational autoencoder-based reinforcement learning) which exploits
variational inference ideas to learn efiiciently and effectively a
solution in a graph-based framework.

***** Opinions
The main down side is that the VARL architecture seems to be quite
complex and specific for combinatorial optimization. That said, it
also shows how variational inference can be effectively used in
combination with reinforcement learning (in the paper REINFOCE was
specifically used.)

**** DONE Robot skill learning in latent space of a deep autoencoder neural network [fn:robot_skill_dae]
     CLOSED: [2022-02-16 Wed 11:18]
[fn:robot_skill_dae]
[[file:background/robot_skill_learning_in_latent_space.pdf]]

***** Summary
      + Gaussian Process Regression (GPR) for statistical learning
      + Shows that Autoencoder-based latent space is more effective
        than PCA-based latent space

      + Tanh as hidden activation function and linear for output
      + Interesting enough the AE with only Linear activation function
        still performs better than PCA (researcher stated that maybe
        it's due to the fact that AE latent space dimensions do not
        have to be orthogonal (interesting!))
      + RL converges faster and it is more stable in latent space with
        respect to DMP space (this is true for both PCA and AE)
      + Moreover RL+AE outperforms RL+PCA! researcher stated that this
        is probably due to the non linearity in AE
      + In particular during the introduction sections it has a lot of
        good references which are worth looking into.

***** Opinions

**** DONE AutoEncoder-based Safe Reinforcement Learning for Power Augmentation in a Lower-limb Exoskeleton [fn:ae_saferl]
     CLOSED: [2022-02-16 Wed 13:19]
[fn:ae_saferl]
[[file:background/AutoEncoder-based_Safe_Reinforcement_Learning_for_Power_Augmentation_in_a_Lower-limb_Exoskeleton.pdf]]

***** Summary
      + GPR used to generate data given few real-world examples.
      + AE both for action and state space reduction.
      + Action and states are DMP
      + MSE as loss function
      + This paper also shows that AE-based latent space make the RL
        learning process faster,safe and more stable.
      + Note: in this paper HMI (Human-Machine Interaction) was a
        central role in the optimization process.

**** DONE The Dreaming Variational Autoencoder for Reinforcement Learning Environments [fn:dreaming_vae]
     CLOSED: [2022-02-16 Wed 13:50]
[fn:dreaming_vae]
[[file:background/dreaming_autoencoder.pdf]]

***** Summary
      + *NOTE* Gaussian distributed policy for initial state-space
        exploration
      + Introduces DVAE architecture
      + Main aims is to model environments with sparse rewards in
        order to perform offline RL.
      + Main problems: if the exploration in the real environment is
        constly. Then this techinique does not behave as the
        environments in the unexplored states.
      + Quite interesting, however, not fitting for highly complex and
        continuos environments where exploration is costly and/or
        risky

**** DONE Deep Variational Reinforcement Learning for POMDPs [fn:dvrl]
     CLOSED: [2022-02-16 Wed 14:56]
[fn:dvrl]
[[file:background/dvrl.pdf]]

***** Summary
      + Defines a new RL method with AE
      + Interesting is that a new method for approximating the ELBO is
        introduced. Using MC methods.
      + *NOTE* Interesting part is that latent space mapping and policy are
        learned toghether. However, policy is update more frequently
        then the latent space. Which stabilize the learning process.
      + Only test in "trivial" environment (Though most of them are
        continuos)
      + Quite interesting, seems too complex and more reasearch most
       be done if we want to use this approch.
      + Contains some usefull references

**** DONE On the use of Deep Autoencoders for Efficient Embedded Reinforcement Learning [fn:dae_efficient_rl]
     CLOSED: [2022-02-16 Wed 15:38]
[fn:dae_efficient_rl]
[[file:background/deep_ae+rl.pdf]]

***** Summary
      + Shows how using AE-based latent space reduce the time of
        convergence. Moreover, it also produce more vaiable policies
        faster.
      + The main downsite with respect to the thesis is that a big
        part of this advantage is due to the fact that images were the
        input to the AE. Of course, this is the main reason why the
        vanilla RL performs drastically worst than the one with the
        Convolutional AE.
      + However, seems another good source of information which again
        shows that AE-based latent space increase the RL performance
        and decrease the time of convergence.
**** DONE DARLA: Improving Zero-Shot Transfer in Reinforcement Learning [fn:darla]
     CLOSED: [2022-02-18 Fri 09:42]
[fn:darla]
[[file:background/DARLA.pdf]]

***** Summary
      + REALLY INTERESTING PAPER!
      + formally shows how to do zero-shot transfer on MDPs
      + introduces this new concept DARLA (DisentAngle Rapresentation
        Learning Agent)
      + 3 steps:
	+ Learn to see (Train the AE with some fixed policy )
	  *Crucial* the distribution of data collected in this phase
          must be as variagate as possible in order to train the AE
          appropriately.
	+ Learn to act
	  Train the RL agent on the source domain using the latent
          space of the AE
	+ Transfer
	  Test the RL agent on the target domain without anymore
          fine-tuning
      + Uses \beta-VAE which aims to force the learning of a
        disentangled representation

***** Resources
[[http://proceedings.mlr.press/v70/higgins17a.html]]



** Old ideas

*** DONE Deep reinforcement learning for modeling human locomotion control in neuromechanical simulation [fn:drl_human_locomotion]
CLOSED: [2022-01-21 Fri 15:27]
[fn:drl_human_locomotion]
[[file:background/oldideas/Deep_reiforcement_learning_for_modeling_human_locomotion_in_neuromechanical_sim.pdf]]

**** Summary
     review paper, it introduces in general the topic. it illustrate
     previous methodology and then it moves on Deep Rl. It talks about
     Learn to Move competition and the different techniques used in
     that competition. Finally some future directions.

**** Opinions
     This paper is really interesting in particular the part about
     Learn to Move and future directions.

**** Ideas
     It suggest imitation learning and hierarchical learning

** TODO Old work

*** Level ground walking for  healthy and transfemoral amputee models. Deep reinforcement learning with phasic policy gradient optimization [fn:ppg]
[fn:ppg]
[[file:background/oldwork/PPG.pdf]]

*** Deep reinforcement learning for physics-based musculoskeletal model of a transfemoral amputee with a prothesis walking on uneven terrain [fn:uneven_terrain]
[fn:uneven_terrain]
[[file:background/oldwork/Uneventerrain.pdf]]

*** Deep reinforcement learning for physics-based musculoskeletal simulations of healthy subjects and transfemoral protheses' users during normal walking [fn:init_thesis]
[fn:init_thesis]
[[file:background/oldwork/initThesis.pdf]]
[[file:background/oldwork/initThesis2.pdf]]

*** Learning to walk: Phasic Policy Gradient for healthy and impaired musculoskeletal models [fn:ppg_carloni]
[fn:ppg_carloni]
[[file:background/oldwork/PPG_carloni.pdf]]

*** Evaluating Deep Reinforcement Learning Algorithms for Physics-Based Musculoskeletal Transfemoral Model with a Prosthetic Leg Performing Ground-Level Walking [fn:shikha]
[fn:shikha]
[[file:background/oldwork/shikha.pdf]]

*** Deep Reinforcement Learning for Physics-based Musculoskeletal Simulations of Transfemoral Prosthesis' Users during the Transition between Normal Walking and Stairs Ascending [fn:stairs]
[fn:stairs]
[[file:background/oldwork/stairs.pdf]]

*** Testing For Generality Of A Proximal Policy Optimiser For Advanced Human Locomotion Beyond Walking [fn:generality_ppo]
[fn:generality_ppo]
[[file:background/oldwork/generality.pdf]]

* Presentations
** First one
#+BEGIN_EXPORT html
<iframe src="resources/init_pres.pdf" width="100%" style="height:50em" align="center"> </iframe>
#+END_EXPORT

* INPROGRESS Code
Visit this file to see the current documentation
#+begin_warning
*WORK IN PROGRESS*  FIXME the documentation is in progress and it can be
 potentially not up to date
#+end_warning
[[file:thesis/docs/build/html/index.html]]

** REPOS
[[https://github.com/vbotics/rug-opensim-rl][vbotics repo]]

[[https://github.com/vimmoos/autoencoders][autoencoders code repo]]

[[https://github.com/vimmoos/thesis][thesis repo]]
* Contacts
** Massimiliano Falzari
m.falzari@student.rug.nl
** Professor
  + Raffealla carloni
  + SkypeID rafficar
  + BlueJeans https://bluejeans.com/821650990  id number = 821650990
** Other Students
  + c.m.sreedhara@student.rug.nl
  + B.N.Ogum@student.rug.nl -> Master student for dim red
  + Chadan

* Todos
** Code
*** DONE Vanilla Autoencoder
CLOSED: [2022-01-21 Fri 15:25]
*** DONE VAE
CLOSED: [2022-01-21 Fri 15:25]
*** DONE AVB
CLOSED: [2022-01-21 Fri 15:25]
*** DONE logging
CLOSED: [2022-01-21 Fri 15:25]
*** DONE tensorboard
CLOSED: [2022-01-21 Fri 15:25]
*** DONE data loader
CLOSED: [2022-01-21 Fri 15:26]
*** DONE cross validation
CLOSED: [2022-01-21 Fri 15:26]
*** DONE parse config
CLOSED: [2022-01-21 Fri 15:26]
*** DONE writer
CLOSED: [2022-01-21 Fri 15:26]
**** Csv writer
**** tensorboard writer
*** DONE validate with different loss
CLOSED: [2022-01-21 Fri 15:26]
*** DONE Create config parser module
CLOSED: [2022-01-21 Fri 15:26]
*** HOLD Dict -> nametuple
*** INPROGRESS Graph module
*** HOLD Collect Data from simulation
** Paper
*** INPROGRESS Review all papers
*** DONE Why we use autoencoders
    CLOSED: [2022-02-18 Fri 13:56]
